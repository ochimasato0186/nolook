#!/usr/bin/env python3
"""
Working Ask API Server
Replaces the complex ask.py with a simple, working implementation
"""
import os
import sys
import random
import json
from datetime import datetime
from pathlib import Path

sys.path.insert(0, os.path.abspath('.'))

from dotenv import load_dotenv
load_dotenv()

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional

app = FastAPI(title="No Look Ask API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class AskRequest(BaseModel):
    prompt: str
    style: Optional[str] = "buddy"
    followup: bool = False
    class_id: Optional[str] = None
    selected_emotion: Optional[str] = None

# AI response templates for fallback
TEMPLATE_RESPONSES = {
    "æ¥½ã—ã„": [
        "ãã‚Œã¯ç´ æ™´ã‚‰ã—ã„ã§ã™ã­ï¼ãã®æ¥½ã—ã„æ°—æŒã¡ã‚’å¤§åˆ‡ã«ã—ã¦ãã ã•ã„ã€‚",
        "ã¨ã¦ã‚‚è‰¯ã„æ°—åˆ†ã§ã™ã­ï¼ä½•ãŒãã‚“ãªã«æ¥½ã—ã‹ã£ãŸã®ã§ã™ã‹ï¼Ÿ",
        "æ¥½ã—ã„æ™‚é–“ã‚’éã”ã›ã¦ã„ã‚‹ã‚ˆã†ã§è‰¯ã‹ã£ãŸã§ã™ï¼"
    ],
    "æ‚²ã—ã„": [
        "è¾›ã„æ°—æŒã¡ã‚’è©±ã—ã¦ãã‚Œã¦ã‚ã‚ŠãŒã¨ã†ã€‚ä¸€äººã˜ã‚ƒãªã„ã‚ˆã€‚",
        "ãã†ã„ã†æ—¥ã‚‚ã‚ã‚Šã¾ã™ã­ã€‚ã‚†ã£ãã‚Šä¼‘ã‚“ã§ãã ã•ã„ã€‚",
        "å¤§å¤‰ã§ã—ãŸã­ã€‚ç„¡ç†ã—ãªã„ã§ãã ã•ã„ã­ã€‚"
    ],
    "æ€’ã‚Š": [
        "ãã®æ°—æŒã¡ã€ã‚ˆãåˆ†ã‹ã‚Šã¾ã™ã€‚æ€’ã‚‹ã®ã¯è‡ªç„¶ãªåå¿œã§ã™ã€‚",
        "ã‚¤ãƒ©ã‚¤ãƒ©ã—ã¾ã™ã‚ˆã­ã€‚æ·±å‘¼å¸ã—ã¦è½ã¡ç€ãã¾ã—ã‚‡ã†ã€‚",
        "æ€’ã‚Šã‚’æ„Ÿã˜ã‚‹ã®ã¯å½“ç„¶ã§ã™ã€‚ã©ã†ã—ã¾ã—ãŸã‹ï¼Ÿ"
    ],
    "ä¸å®‰": [
        "ä¸å®‰ãªæ°—æŒã¡ã€åˆ†ã‹ã‚Šã¾ã™ã€‚ä¸€ç·’ã«è€ƒãˆã¾ã—ã‚‡ã†ã€‚",
        "å¿ƒé…äº‹ãŒã‚ã‚‹ã‚“ã§ã™ã­ã€‚è©±ã›ã‚‹ç¯„å›²ã§æ•™ãˆã¦ãã ã•ã„ã€‚",
        "ä¸å®‰ã«ãªã‚‹ã®ã¯è‡ªç„¶ã§ã™ã€‚ä¸€æ­©ãšã¤é€²ã¿ã¾ã—ã‚‡ã†ã€‚"
    ],
    "ã—ã‚“ã©ã„": [
        "æœ¬å½“ã«ãŠç–²ã‚Œæ§˜ã§ã™ã€‚ç„¡ç†ã—ãªã„ã§ãã ã•ã„ã­ã€‚",
        "å¤§å¤‰ãªæ™‚æœŸã§ã™ã­ã€‚å°‘ã—ä¼‘æ†©ã—ã¾ã›ã‚“ã‹ï¼Ÿ",
        "ã—ã‚“ã©ã„æ™‚ã¯èª°ã‹ã«é ¼ã£ã¦ã‚‚å¤§ä¸ˆå¤«ã§ã™ã‚ˆã€‚"
    ],
    "ä¸­ç«‹": [
        "ãŠè©±ã—ã‚’èã‹ã›ã¦ãã‚Œã¦ã‚ã‚ŠãŒã¨ã†ã€‚",
        "ãã†ã§ã™ã­ã€ã‚ˆãåˆ†ã‹ã‚Šã¾ã™ã€‚",
        "ãªã‚‹ã»ã©ã€ãã†ã„ã†ã“ã¨ãªã‚“ã§ã™ã­ã€‚"
    ]
}

def simple_emotion_analysis(text: str) -> str:
    """Simple emotion detection based on keywords"""
    text = text.lower()
    
    if any(word in text for word in ["æ¥½ã—ã„", "å¬‰ã—ã„", "å¹¸ã›", "æœ€é«˜", "ç´ æ™´ã‚‰ã—ã„", "è‰¯ã„"]):
        return "æ¥½ã—ã„"
    elif any(word in text for word in ["æ‚²ã—ã„", "è¾›ã„", "å¯‚ã—ã„", "è½ã¡è¾¼ã‚€"]):
        return "æ‚²ã—ã„"
    elif any(word in text for word in ["æ€’", "ã‚¤ãƒ©ã‚¤ãƒ©", "è…¹ç«‹ã¤", "ãƒ ã‚«ã¤ã"]):
        return "æ€’ã‚Š"
    elif any(word in text for word in ["ä¸å®‰", "å¿ƒé…", "æ€–ã„", "ç·Šå¼µ"]):
        return "ä¸å®‰"
    elif any(word in text for word in ["ç–²ã‚Œ", "ã—ã‚“ã©ã„", "å¤§å¤‰", "ãã¤ã„"]):
        return "ã—ã‚“ã©ã„"
    else:
        return "ä¸­ç«‹"

def get_llm_response(prompt: str, emotion: str) -> Optional[str]:
    """Get response from OpenAI LLM"""
    try:
        from openai import OpenAI
        
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("No OpenAI API key found")
            return None
            
        client = OpenAI(api_key=api_key)
        
        system_prompt = (
            "ã‚ãªãŸã¯æ—¥æœ¬ã®å­¦ç”Ÿã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹å„ªã—ã„ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
            "çŸ­ãæ¸©ã‹ã„è¿”äº‹ã‚’ã—ã¦ãã ã•ã„ã€‚1-2æ–‡ã§ã€120æ–‡å­—ä»¥å†…ã§ãŠé¡˜ã„ã—ã¾ã™ã€‚"
            f"å­¦ç”Ÿã®æ„Ÿæƒ…ã¯ã€Œ{emotion}ã€ã§ã™ã€‚ãã®æ„Ÿæƒ…ã«å¯„ã‚Šæ·»ã£ãŸè¿”äº‹ã‚’ã—ã¦ãã ã•ã„ã€‚"
        )
        
        response = client.chat.completions.create(
            model=os.getenv("NOLOOK_LLM_MODEL", "gpt-4o-mini"),
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=100,
        )
        
        result = response.choices[0].message.content.strip()
        print(f"âœ… LLM Response: {result}")
        return result
        
    except Exception as e:
        print(f"âŒ LLM Error: {e}")
        return None

def save_response_data(prompt: str, response: str, emotion: str, used_llm: bool):
    """Save response data to JSON file"""
    try:
        data_dir = Path("data/ai_responses")
        data_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now()
        filename = f"{timestamp.strftime('%Y-%m-%d')}_responses.json"
        filepath = data_dir / filename
        
        # Load existing data or create new
        if filepath.exists():
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
        else:
            data = {"responses": []}
        
        # Add new response
        data["responses"].append({
            "timestamp": timestamp.isoformat(),
            "user_input": prompt,
            "ai_response": response,
            "emotion": emotion,
            "used_llm": used_llm
        })
        
        # Save back to file
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
            
        print(f"âœ… Saved response to {filepath}")
        
    except Exception as e:
        print(f"âŒ Save error: {e}")

@app.post("/ask")
def ask(request: AskRequest):
    """Main ask endpoint"""
    try:
        print(f"ğŸ“ Received request: {request.prompt}")
        
        # Emotion analysis
        emotion = simple_emotion_analysis(request.prompt)
        print(f"ğŸ˜Š Detected emotion: {emotion}")
        
        # Try LLM first
        llm_response = get_llm_response(request.prompt, emotion)
        
        if llm_response and len(llm_response.strip()) > 0:
            # LLM success
            response_text = llm_response
            used_llm = True
            llm_reason = None
            print("ğŸ¤– Using LLM response")
        else:
            # Fallback to template
            templates = TEMPLATE_RESPONSES.get(emotion, TEMPLATE_RESPONSES["ä¸­ç«‹"])
            response_text = random.choice(templates)
            used_llm = False
            llm_reason = "llm_failed_or_empty"
            print("ğŸ“‹ Using template response")
        
        # Add followup if requested
        if request.followup:
            response_text += " ã‚ˆã‹ã£ãŸã‚‰ã€ã‚‚ã†å°‘ã—è©³ã—ãæ•™ãˆã¦ï¼Ÿ"
        
        # Save data
        save_response_data(request.prompt, response_text, emotion, used_llm)
        
        # Create emotion labels (simple version)
        labels = {emotion_name: 1.0 if emotion_name == emotion else 0.0 
                 for emotion_name in ["æ¥½ã—ã„", "æ‚²ã—ã„", "æ€’ã‚Š", "ä¸å®‰", "ã—ã‚“ã©ã„", "ä¸­ç«‹"]}
        
        result = {
            "reply": response_text,
            "emotion": emotion,
            "labels": labels,
            "used_llm": used_llm,
            "llm_reason": llm_reason,
            "style": request.style,
            "followup": request.followup
        }
        
        print(f"âœ… Response: {result}")
        return result
        
    except Exception as e:
        print(f"âŒ Ask endpoint error: {e}")
        import traceback
        traceback.print_exc()
        
        # Emergency fallback
        return {
            "reply": "ã™ã¿ã¾ã›ã‚“ã€ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
            "emotion": "ä¸­ç«‹",
            "labels": {"æ¥½ã—ã„": 0.0, "æ‚²ã—ã„": 0.0, "æ€’ã‚Š": 0.0, "ä¸å®‰": 0.0, "ã—ã‚“ã©ã„": 0.0, "ä¸­ç«‹": 1.0},
            "used_llm": False,
            "llm_reason": f"error: {str(e)}",
            "style": request.style or "buddy",
            "followup": request.followup
        }

@app.get("/")
def root():
    return {"message": "No Look Ask API is running", "status": "ok"}

@app.get("/health")
def health():
    """Health check endpoint"""
    return {"status": "healthy", "llm_available": bool(os.getenv("OPENAI_API_KEY"))}

if __name__ == "__main__":
    import uvicorn
    print("ğŸš€ Starting No Look Ask API Server...")
    print(f"ğŸ”‘ OpenAI API Key: {'Available' if os.getenv('OPENAI_API_KEY') else 'Missing'}")
    uvicorn.run(app, host="0.0.0.0", port=8000)